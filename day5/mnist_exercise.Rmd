---
title: "MNIST with R Keras"
subtitle: "Computer age statistical inference, University of Helsinki"
author: "Tuomo Nieminen"
date: "May 4, 2018"
output: 
  github_document:
    html_preview: false
    toc: true
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## About

In this exercise you will learn how to use the r package *Keras* to build and train a neural network model which learns to classify images of handwritten digits. The model is trained using Google's deep learning library *Tensorflow* for which Keras provides a high-level interface. This exercise is based on the example given in https://keras.rstudio.com/.

## Prerequisities

**Step 1**: install the r package *keras*.

```{r, eval = FALSE}
install.packages("keras")
```

**Step 2**: Install all dependecies for keras.

Keras provides a high-level interface to Google's deep learning library Tensorflow. Keras is originally a Python package and the R version of Keras depends on both Python Keras and Tensorflow. The function `install_keras()` in the R Keras package handles these dependencies. The function will alert you of dependencies you need to install manually (on windows you will need Anaconda 3.x). For more information see the help page ([link](https://keras.rstudio.com/reference/install_keras.html), note that you can safely ignore the GPU installation part). 

```{r, eval = FALSE}
keras::install_keras()
```

## Instructions

This is a tutorial-like exercise where most of the R code needed is given and usable as-is. However, parts in the code marked with "CHANGE ME!" are meant to be changed. Fire up your favourite R editor (RStudio is highly recommended) and follow along!


# MNIST Data 

### Load the MNIST data

Load the mnist dataset as `X` and `y` r-objects.


```{r}
library(keras)

mnist <- dataset_mnist()
X <- mnist$train$x
y <- mnist$train$y
```


*NOTE: If you encounter an 'unkown url type' error, open command line and do:*  
```
source ~/anaconda3/envs/r-tensorflow/bin/activate
python
from keras.datasets import mnist
mnist.load_data()
```  
*Then repeat the above R commands.*


### Explore the data

Explore the objects with `str()` and/or other suitable R commands. What are their dimensionalities? 

```{r}
str(X) # images 
str(y) # labels 
```

```{r}
dim(X)
dim(y)
```


Plot a random sample of 16 images in grey scale using the `image()` function. Is there something problematic about the images from a human perspective? How about from a computer/algorithmic perspective?

```{r}
par(mfrow = c(4, 4), mar = c(0,0,0,0), yaxt = "n", xaxt = "n")

for(i in sample(1:nrow(X), 16))
  image(X[i, , ], col = grey(seq(1,0, length = 256)))
```


### Reshaping

Reshape both the X and the y arrays by collapsing X into a matrix and by expanding y to a hot-one encoded matrix.

```{r}
N <- dim(X)[1]
NC <- dim(X)[2] * dim(X)[3]

X_ <- array_reshape(X, c(N, NC)) # reshape X to (N, 784) matrix

X_ <- X_ / 255 # rescale to 0...1

y_ <- to_categorical(y) # hot-one encode y
```

Now look at the first couple observations in `y_`. What is the format of the labels? What is hot-one encoding?

```{r}
"CHANGE ME!"(y_)
```


### Train and test split

Split the data into train and test sets (80 - 20 split).  

```{r}
train_indices <- sample(1:N, N*0.8)

x_train <- X_[train_indices , ]
y_train <- y_[train_indices, ]

x_test <- X_[-train_indices , ]
y_test <- y_[-train_indices, ]
```

# Neural Network Model for Classification

### Define the structure for a neural network

Use the Keras sequential model to define a neural network model. 

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(input_shape = c("CHANGE ME!"), units = 64, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')

```

*hint*:  
The input shape should correspond to the shape of the input data, ignoring the sample size dimension. e.g. if our data is a matrix with 3 features, this should be a length one integer vecor with the value 3.

### Compile the model

Define the loss function and the optimizer algorithm.

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_sgd(),
  metrics = c('accuracy')
)
```

### Train the model

Train the model by inputting data in batches of 480 observations, passing through the complete dataset 20 times.

```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = "CHANGE ME!", 
  batch_size = "CHANGE ME!", 
  validation_data = list(x_test, y_test)
)
```


# Model Performance

### Training history

```{r}
history
```

Plot the accuracy and loss for each pass of the complete dataset, for both train and test data.

```{r}
plot(history)
```


### Predicting

Use the model to predict labels on the test data. The predictions are given as a vector of probabilities for each label ("soft" predictions).

```{r}
predicted_proba <- model %>% predict(x_test)
```

A simple way to evaluate model performance is by converting to "hard" predictions and then comparing the predictions to the actual labels. Convert both the probability predictions and the hot-one encoded test labels to vectors of label indeces.

```{r}
predicted_label <- max.col(predicted_proba)
true_label <- max.col(y_test)
```

### Confusion matrix

A confusion matrix is a cross-tabulation of the true labels and the predicted labels. Compute that matrix.

```{r}
confusion <- "CHANGE ME!"
confusion
```

*hint*: `table()`

### Accuracy

Accuracy is the proportion of correctly classified labels. Use the confusion martix to compute the accuracy. Compare it to the accurace reported in the history object.

```{r}
acc <- sum(diag(confusion)) / sum(confusion)
acc
history
```


# Further exercises

- Try out a more complicated network by adding an extra layer
- Add a convolutional layer and a max pooling layer
- Use dropout regularisation to prevent overfitting

