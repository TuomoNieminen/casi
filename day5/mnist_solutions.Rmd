---
title: "MNIST with R Keras (solutions)"
subtitle: "Computer age statistical inference, University of Helsinki"
author: "Tuomo Nieminen"
date: "May 4, 2018"
output: 
  github_document:
    html_preview: false
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```


# MNIST Data 

### Load the MNIST data

Load the mnist dataset as `X` and `y` r-objects.


```{r}
library(keras)

mnist <- dataset_mnist()
X <- mnist$train$x
y <- mnist$train$y
```


### Explore the data

Explore the objects with `str()` or other suitable R commands. What are their dimensionalities? 

```{r}
str(X) # images (n, 28, 28)
str(y) # labels (n)
```

Distribution of labels

```{r}
table(y)
```


Plot a random sample of 16 images in grey scale using the `image()` function.

```{r}
par(mfrow = c(4, 4), mar = c(0,0,0,0), yaxt = "n", xaxt = "n")

for(i in sample(1:nrow(X), 16))
  image(X[i, , ], col = grey(seq(1,0, length = 256)))
```


### Reshaping

Reshape both the X and the y arrays by collapsing X into a matrix and by expanding y to a hot-one encoded matrix.

```{r}
N <- dim(X)[1]
NC <- dim(X)[2] * dim(X)[3]

X_ <- array_reshape(X, c(N, NC)) # reshape X to (N, 784) matrix

X_ <- X_ / 255 # rescale to 0...1

y_ <- to_categorical(y) # hot-one encode y
```

Now look at the first couple observations in `y_`. What is the format of the labels? What is hot-one encoding?

```{r}
colnames(y_) <- 0:9
head(y_) %>% knitr::kable()
```


### Train and test split

Split the data into train and test sets

```{r}
train_indices <- sample(1:N, N*0.8)

x_train <- X_[train_indices , ]
y_train <- y_[train_indices, ]

x_test <- X_[-train_indices , ]
y_test <- y_[-train_indices, ]
```

# Neural Network Model for Classification

### Define the structure for a neural network

Use the Keras sequential model to define a neural network model. The input shape is (n, 784).  

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(input_shape = dim(x_train)[-1], units = 64, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')
```

### Compile the model

Define the loss function and the optimizer algorithm.

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_sgd(),
  metrics = c('accuracy')
)
```

### Train the model

Train the model on 20 epochs by inputting data in batches of 480 observations.

```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 20, 
  batch_size = 480, 
  validation_data = list(x_test, y_test),
  verbose = 2
)
```


# Model Performance

### Training history

```{r}
history
```

Plot the accuracy and loss for each pass of the complete dataset, for both train and test data.

```{r}
plot(history)
```


### Predicting

Use the model to predict labels on the test data. 

```{r}
predicted_proba <- model %>% predict(x_test)
colnames(predicted_proba) <- 0:9
```

Append the true labels

```{r}
predictions_labels <- cbind(round(predicted_proba, 2), label = y[-train_indices])
predictions_labels %>% head() %>% knitr::kable()
```

### Confusion matrix

A confusion matrix is a cross-tabulation of the true labels and the predicted labels. 

To build it, first convert both the probability predictions and the hot-one encoded test labels to vectors of label indeces.

```{r}
predicted_label <- max.col(predicted_proba)
true_label <- max.col(y_test)
```

Cross-tabulate predicted labels and true labels.

```{r}
confusion <- table(true_label, predicted_label)
confusion
```

### Accuracy

Accuracy is the proportion of correctly classified labels. Use the confusion martix to compute the accuracy. Compare it to the accurace reported in the history object.  

```{r}
acc <- sum(diag(confusion)) / sum(confusion)
acc
history
```

