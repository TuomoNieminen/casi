---
title: "MNIST with R Keras (solutions)"
subtitle: "Computer age statistical inference, University of Helsinki"
author: "Tuomo Nieminen"
date: "May 4, 2018"
output: 
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# MNIST Data 

### Load the MNIST data

Load the mnist dataset as `X` and `y` r-objects.


```{r}
library(keras)

mnist <- dataset_mnist()
X <- mnist$train$x
y <- mnist$train$y
```


### Explore the data

Explore the objects with `str()` or other suitable R commands. What are their dimensionalities? 

```{r}
cat("Images: ")
str(X) # images (n, 28, 28)

cat("\nLabels: ")
str(y) # labels (n)

cat("\nDistribution of labels: \n")
table(y) # distribution of labels
```


Plot some of the images. Is there anything strange about the images from a human perspective? How about from a computer/algorithmic perspective?

```{r}
# plot some of the images
par(mfrow = c(4,4), mar = c(0,0,0,0), xaxt = "n", yaxt = "n")
indices <- sample(1:nrow(X_), 16)
for(i in indices)
  image(X[i, , ], col = grey(seq(1,0, length = 256)))
```


### Reshaping

Reshape both the X and the y arrays by collapsing X into a matrix and by expanding y to a hot-one encoded matrix.

```{r}
# reshape the n, 28, 28 array to a n, 28*28 matrix
X_ <- array_reshape(X, c(nrow(X), 784))

# rescale to 0...1
X_ <- X_ / 255

# hot-one encode y
y_ <- to_categorical(y)
```

Now look at the first couple observations in `y_`. What is the format of the labels? What is hot-one encoding?

```{r}
head(y_)
```


### Train and test split

Split the data into train and test sets

```{r}
n <- nrow(X)
train_indices <- sample(1:n, n*0.8)

x_train <- X_[train_indices , ]
y_train <- y_[train_indices, ]

x_test <- X_[-train_indices , ]
y_test <- y_[-train_indices, ]
```

# Neural Network Model for Classification

### Define the structure for a neural network

Use the Keras sequential model to define a neural network model. 

```{r}
# define the structure of a network
model <- keras_model_sequential() 
model %>% 
  layer_dense(input_shape = c(784), units = 64, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')

```

*hint* 
Complete the network definition by defining the input shape of the first dense layer. This should be the shape of the input data, ignoring the sample size dimension. e.g. if our data has 3 features, this should be a length one integer vecor with the value 3.

### Compile the model

```{r}
# define the loss and optimizer algorithm
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_sgd(),
  metrics = c('accuracy')
)
```

### Train the model

Train the model on 20 epochs by inputting data in batches of 480 observations.

```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 20, 
  batch_size = 480, 
  validation_data = list(x_test, y_test),
  verbose = 2
)
```


# Model Performance

### Training history

```{r}
history
```

```{r}
plot(history)
```


### Predicting

Use the model to predict labels on the test data. 

```{r}
predicted_proba <- model %>% predict(x_test)
head(predicted_proba)
```

The predictions are given as a vector of probabilities for each label ("soft" predictions). A simple way to evaluate model performance is by converting to "hard" predictions and then comparing to the actual labels. Convert both the probability predictions and the hot-one encoded test labels to vectors of label indeces.

```{r}
predicted_label <- max.col(predicted_proba)
true_label <- max.col(y_test)
```

### Confusion matrix

A confusion matrix is a cross-tabulation of the true labels and the predicted labels. Compute that matrix.

```{r}
confusion <- table(true_label, predicted_label)
confusion
```

*hint*: `table()`

### Accuracy

Accuracy is the proportion of correctly classified labels. Use the confusion martix to compute the accuracy. Compare it to the accurace reported in the history object.

```{r}
# accuracy
acc <- sum(diag(confusion)) / sum(confusion)
cat("Accuracy: ", acc, "\n\n")
history
```


